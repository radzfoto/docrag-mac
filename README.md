# docrag-mac
LLM document RAG workflow for macos

- This repo is an implementation of a retrieval augmented generation workflow for PDF documents. The workflow is implemented with langchain using ChromaDB for the vector database and MongoDB for the document store.
- There are two separate workflows, Embedding Workflow and Query Workflow. A single configuration file called config.json which located at the root of this project contains all the parameters needed for the project.
- The first workflow generates embeddings from PDF documents and stores them in a vector database with a link to the PDF document chunks that the embedding refers to. This workflow can be run anytime that new PDF documents are added. This workflow is herein called the Embedding Workflow.
- The second worflow performs queries related to the documents using the same embedding model as in the Embedding Workflow. This workflow is herein called the Query Workflow. After the query is passed through the embedding model, the query embedding is used to perform a similarity search in a vector database. The N most similar embeddings are pulled from the vector database, and then filtered down to the K best matches. Parameters N and K are specified in the config.json file.
- A User Interface is implemented to allow users to submit text queries in natural language and display responses.
- There are two LLMs: one for the embedding model and for the responses from the query. The query function uses the embeddings to pull up the respective documents associated with those embedding. These documents are used as the context for the query to be sent to the Response LLM with processes the query with the context and provides a reply to the User Interface.
- For the Embedding Workflow, the PDF documents are divided into chunks and each chunk converted into an embedding. The size of each chunk is a programmable amount initially set to 100 tokens. The chunking is also overlapped by a programmable amount, initially set to 20%. The chunk size and chunk overlap are specified in the config.json file.
- The PDF documents are located in a directory tree. The location of the directory tree is read from a JSON configuration file called config.json that is itself located in the root directory of this repo.
- The vector database and MongoDB database locations are also specified in the JSON configuration file, config.json. The two databases are normally in different directories, and often in different servers, so a full URL is provided for each database and the startup command for each database is also provided.
- The embedding model and the response LLMs are served using independent models servers where the LLMs may run on independent GPUs or AI accelerators and, even, on separate servers. The location and startup command of the models servers is in the config.json file.
- All servers will run Linux. Assume Ubuntu 22.04 distribution is installed on the servers.
- Write software in Python, shell scripts, and configuration files as necessary to implement everything in this document. In addition, bundle the software implementation into separate microservices as makes the most sense.
